{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_dir = '/Users/christopher/Documents/unirepsCache'\n",
    "cache_dir = '/net/scratch2/chriswolfram/hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'meta-llama/Llama-3.2-1B'\n",
    "model_name = 'google/gemma-2-27b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f42e64d4a44c02958368ae6f5a65a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map='auto', cache_dir=cache_dir)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use split slicing and maybe shuffling+streaming to extract samples from large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('stanfordnlp/imdb', cache_dir=cache_dir)\n",
    "dataset = dataset['test'].take(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0055, -0.0071,  0.0009,  ...,  0.0044,  0.0036,  0.0037],\n",
       "        [-0.0318,  0.0242,  0.0067,  ...,  0.0007,  0.0008, -0.0130],\n",
       "        [ 0.0048, -0.0054, -0.0048,  ...,  0.0069, -0.0050,  0.0040],\n",
       "        ...,\n",
       "        [-0.0077,  0.0068, -0.0240,  ...,  0.0234,  0.0228, -0.0018],\n",
       "        [ 0.0109,  0.0115, -0.0016,  ..., -0.0034, -0.0011,  0.0138],\n",
       "        [ 0.0058, -0.0071,  0.0007,  ...,  0.0043,  0.0035,  0.0038]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664d89a174141f9aadd0f21a26c1543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    last_token_indices = attention_mask.sum(dim=-1) - 1\n",
    "    embeddings = model_out.hidden_states[-1][torch.arange(input_ids.size(0)), last_token_indices]\n",
    "    \n",
    "    return {'embeddings': embeddings}\n",
    "\n",
    "train_embeddings = dataset.map(compute_embeddings, batched=True, batch_size=64)\n",
    "train_embeddings.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(dataset['text'][0], padding='longest', return_tensors='pt')\n",
    "input_ids = tokens.input_ids.to(model.device)\n",
    "attention_mask = tokens.attention_mask.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9035e-03,  3.4298e-03,  1.1937e-02,  ..., -6.0098e-03,\n",
       "         -5.7379e-03, -4.3657e-03],\n",
       "        [-1.2339e-02,  1.1494e-02, -1.5648e-02,  ...,  4.3412e-03,\n",
       "         -1.2122e-02,  1.6042e-03],\n",
       "        [ 1.9405e-03,  6.4132e-03, -1.8607e-02,  ...,  9.9847e-03,\n",
       "         -1.8577e-02, -5.6446e-03],\n",
       "        ...,\n",
       "        [-1.3252e-01, -3.6015e-02, -4.6515e-03,  ..., -1.4342e-01,\n",
       "          3.2569e-02,  9.9657e-02],\n",
       "        [-1.3976e-01,  3.2463e-02,  1.0859e-01,  ..., -1.3966e-01,\n",
       "          2.4561e-03,  1.0292e-01],\n",
       "        [-4.1964e-01,  2.4690e+00,  1.1171e+00,  ..., -2.7177e+00,\n",
       "         -2.7629e+00,  7.1740e-01]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,:,:].float().mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9035e-03,  3.4298e-03,  1.1937e-02,  ..., -6.0098e-03,\n",
       "         -5.7379e-03, -4.3657e-03],\n",
       "        [-1.2339e-02,  1.1494e-02, -1.5648e-02,  ...,  4.3412e-03,\n",
       "         -1.2122e-02,  1.6042e-03],\n",
       "        [ 1.9405e-03,  6.4132e-03, -1.8607e-02,  ...,  9.9847e-03,\n",
       "         -1.8577e-02, -5.6446e-03],\n",
       "        ...,\n",
       "        [-1.3252e-01, -3.6015e-02, -4.6515e-03,  ..., -1.4342e-01,\n",
       "          3.2569e-02,  9.9657e-02],\n",
       "        [-1.3976e-01,  3.2463e-02,  1.0859e-01,  ..., -1.3966e-01,\n",
       "          2.4561e-03,  1.0292e-01],\n",
       "        [-4.1964e-01,  2.4690e+00,  1.1171e+00,  ..., -2.7177e+00,\n",
       "         -2.7629e+00,  7.1740e-01]], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,:,:].float().sum(1) / (torch.stack(model_output.hidden_states)[:,0,:,:].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just don't use batching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb9ce87257a4a94a67405ee2a962c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    layer_token_embeddings = torch.stack(model_out.hidden_states)[:,0].float()\n",
    "\n",
    "    layer_last_embeddings = layer_token_embeddings[-1].cpu()\n",
    "    layer_mean_embeddings = layer_token_embeddings.mean(1).cpu()\n",
    "\n",
    "    return {'layer_last_embeddings': layer_last_embeddings, 'layer_mean_embeddings': layer_mean_embeddings}\n",
    "\n",
    "train_embeddings_single = dataset.take(256).map(compute_embeddings, new_fingerprint='test_fingerprint', load_from_cache_file=False)\n",
    "train_embeddings_single.set_format('torch')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126c8f0b57ce44fb88a03c5507e24208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    input_layer_token_embeddings = torch.stack(model_out.hidden_states).permute(1,0,2,3).float()\n",
    "\n",
    "    last_token_indices = attention_mask.sum(-1) - 1\n",
    "    layer_last_embeddings = input_layer_token_embeddings[torch.arange(input_layer_token_embeddings.shape[0]), :, last_token_indices].cpu()\n",
    "    layer_mean_embeddings = ((input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)).cpu()\n",
    "\n",
    "    return {'layer_last_embeddings': layer_last_embeddings, 'layer_mean_embeddings': layer_mean_embeddings}\n",
    "\n",
    "train_embeddings_batch = dataset.map(compute_embeddings, batched=True, batch_size=4, new_fingerprint='test_fingerprint', load_from_cache_file=False)\n",
    "train_embeddings_batch.set_format('torch')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9249e-04,  3.6607e-03,  1.0899e-02,  ..., -3.2817e-03,\n",
       "         -4.6034e-03, -4.7947e-03],\n",
       "        [-1.0231e-02,  1.3209e-02, -1.7298e-02,  ...,  3.0661e-03,\n",
       "         -1.9254e-02,  8.4557e-04],\n",
       "        [-8.9790e-04,  8.7126e-03, -2.5084e-02,  ...,  1.0079e-02,\n",
       "         -2.1291e-02, -6.6319e-03],\n",
       "        ...,\n",
       "        [-1.0436e-01,  6.7000e-02, -4.4920e-04,  ..., -8.2969e-02,\n",
       "          1.0784e-01,  1.7436e-02],\n",
       "        [-1.5864e-01,  9.7246e-02,  7.4547e-02,  ..., -5.5770e-02,\n",
       "          1.5204e-01, -9.8039e-02],\n",
       "        [-3.5685e-01,  3.0189e+00,  9.5680e-01,  ..., -1.9110e+00,\n",
       "         -1.6960e+00, -1.8769e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings_batch[-1]['layer_mean_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9249e-04,  3.6607e-03,  1.0899e-02,  ..., -3.2817e-03,\n",
       "         -4.6034e-03, -4.7947e-03],\n",
       "        [-1.0236e-02,  1.3211e-02, -1.7296e-02,  ...,  3.0657e-03,\n",
       "         -1.9261e-02,  8.5039e-04],\n",
       "        [-8.8779e-04,  8.7090e-03, -2.5072e-02,  ...,  1.0049e-02,\n",
       "         -2.1277e-02, -6.6151e-03],\n",
       "        ...,\n",
       "        [-1.0492e-01,  6.6615e-02, -8.0678e-04,  ..., -8.2719e-02,\n",
       "          1.0788e-01,  1.7229e-02],\n",
       "        [-1.5912e-01,  9.6377e-02,  7.3573e-02,  ..., -5.5962e-02,\n",
       "          1.5189e-01, -9.8142e-02],\n",
       "        [-3.6279e-01,  3.0147e+00,  9.5616e-01,  ..., -1.9135e+00,\n",
       "         -1.6963e+00, -1.9119e-01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings_single[-1]['layer_mean_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=tokenizer(train_embeddings_single['text'][-1], return_tensors='pt').input_ids.to(model.device), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=tokenizer('Hello, World!', return_tensors='pt').input_ids.to(model.device), output_hidden_states=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-11.1768,   9.2193,   3.9765,  ...,   6.9122,  -2.9723,  -0.1966],\n",
       "        [  3.1883,   1.1481,   0.0692,  ...,  -2.1562,  -2.6575,   2.5491],\n",
       "        [  2.0031,  -0.4243,  -3.2966,  ...,  -0.6648,   2.6865,  -3.3028],\n",
       "        [  0.5020,   0.0852,   3.9875,  ...,   1.1158,  -6.8943,  10.7729],\n",
       "        [ -2.2840,  -0.4689,  -0.0650,  ...,   3.3670,   1.4833,  -4.0887]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8945e-01,  9.0832e-01,  6.7385e-01,  ...,  2.2225e-01,\n",
       "          3.7783e-01,  4.7995e-02],\n",
       "        [-1.6288e-02,  4.9965e-01,  4.5269e-01,  ...,  1.5644e-01,\n",
       "          2.3224e-01, -1.3287e-01],\n",
       "        [-1.0866e-01,  3.9563e-01,  3.7154e-01,  ...,  2.8733e-01,\n",
       "          2.9141e-02, -1.6444e-01],\n",
       "        ...,\n",
       "        [ 6.9677e+01, -6.7399e+01,  9.0780e+01,  ..., -2.2300e+01,\n",
       "         -3.5789e+01,  5.2989e+01],\n",
       "        [-2.5485e+00, -9.9558e+01,  4.8827e+01,  ..., -4.1079e+01,\n",
       "         -8.1324e+01,  7.6477e+01],\n",
       "        [-1.5535e+00,  1.9119e+00,  9.3433e-01,  ...,  1.7148e+00,\n",
       "         -1.6708e+00,  1.1468e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].float().mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5535,  1.9119,  0.9343,  ...,  1.7148, -1.6708,  1.1468],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).mean(2)[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5547, -1.5234,  2.7812,  ..., -0.6445,  0.6094, -1.0469],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings[0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 98, 2048])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_output.hidden_states).permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00, -6.1035e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5259e-05,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.0518e-05,  1.5259e-05,  2.4414e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.0518e-05],\n",
       "        ...,\n",
       "        [-9.7656e-04,  0.0000e+00,  4.8828e-04,  ..., -9.7656e-04,\n",
       "         -4.8828e-04,  0.0000e+00],\n",
       "        [ 0.0000e+00,  9.7656e-04, -1.2207e-04,  ...,  1.9531e-03,\n",
       "         -1.2207e-03,  8.5449e-04],\n",
       "        [-3.9062e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.9062e-03]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings['layer_mean_embeddings'][-1] - torch.stack(model_output.hidden_states)[:,0].mean(1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 161, 2048])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1414e-02, -2.2278e-03, -1.1292e-03,  ..., -1.7944e-02,\n",
       "         -2.6855e-03, -2.1729e-02],\n",
       "        [ 2.0142e-03,  4.8828e-03, -1.0400e-01,  ..., -2.1729e-02,\n",
       "         -2.8809e-02, -3.7354e-02],\n",
       "        [ 7.7515e-03, -5.6152e-02, -1.5430e-01,  ...,  5.4932e-02,\n",
       "         -3.9551e-02, -6.4941e-02],\n",
       "        ...,\n",
       "        [-9.5703e-02,  3.2227e-01, -5.9766e-01,  ..., -7.6562e-01,\n",
       "         -7.1289e-02, -5.2246e-02],\n",
       "        [ 1.9141e-01,  4.3359e-01, -5.6641e-01,  ..., -6.3281e-01,\n",
       "         -9.3262e-02, -6.0547e-02],\n",
       "        [ 2.5312e+00,  3.9062e+00,  2.3730e-01,  ..., -4.7188e+00,\n",
       "         -4.8438e+00, -1.0781e+00]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9182e-04,  3.4027e-03,  1.2512e-02,  ..., -8.8501e-04,\n",
       "         -6.8970e-03, -5.7068e-03],\n",
       "        [-1.2085e-02,  8.4229e-03, -1.9653e-02,  ...,  9.2773e-03,\n",
       "         -2.4048e-02,  4.2419e-03],\n",
       "        [ 1.1292e-03,  3.9673e-03, -2.4658e-02,  ...,  1.7700e-02,\n",
       "         -3.5400e-02, -3.6926e-03],\n",
       "        ...,\n",
       "        [ 3.4668e-02,  1.6309e-01, -1.3184e-01,  ..., -9.6680e-02,\n",
       "         -1.6724e-02,  7.3730e-02],\n",
       "        [ 9.0332e-02,  2.1484e-01, -7.2754e-02,  ..., -7.2754e-02,\n",
       "         -1.3281e-01, -2.6611e-02],\n",
       "        [ 1.3125e+00,  3.5312e+00,  6.1719e-01,  ..., -2.7969e+00,\n",
       "         -3.2812e+00, -1.4648e-01]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'model_out': model_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_model_out(dataset.take(10))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = out['input_ids'].cpu()\n",
    "attention_mask = out['attention_mask'].cpu()\n",
    "model_out = out['model_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_indices = attention_mask.sum(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_out.hidden_states).cpu().permute(1,0,2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_out.hidden_states).permute(1,0,2,3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_last_embedding = input_layer_token_embeddings[torch.arange(input_layer_token_embeddings.shape[0]), :, last_token_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 452])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 452])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 452, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([315, 279, 146, 452, 160, 218, 361, 208, 177, 216])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_mean_embeddings = (input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9073e-03,  3.4180e-03,  1.1902e-02,  ..., -5.9814e-03,\n",
       "          -5.7068e-03, -4.3640e-03],\n",
       "         [-1.2329e-02,  1.1475e-02, -1.5625e-02,  ...,  4.3640e-03,\n",
       "          -1.2085e-02,  1.6098e-03],\n",
       "         [ 1.9531e-03,  6.3782e-03, -1.8555e-02,  ...,  1.0010e-02,\n",
       "          -1.8433e-02, -5.5847e-03],\n",
       "         ...,\n",
       "         [-1.3086e-01, -3.5156e-02, -4.4861e-03,  ..., -1.4258e-01,\n",
       "           3.2959e-02,  9.8633e-02],\n",
       "         [-1.3965e-01,  3.3691e-02,  1.0840e-01,  ..., -1.3867e-01,\n",
       "           2.4261e-03,  1.0107e-01],\n",
       "         [-4.1406e-01,  2.4688e+00,  1.1172e+00,  ..., -2.7031e+00,\n",
       "          -2.7656e+00,  7.1094e-01]],\n",
       "\n",
       "        [[-2.7924e-03,  2.7618e-03,  1.0254e-02,  ..., -3.5400e-03,\n",
       "          -4.1809e-03, -3.6621e-03],\n",
       "         [-1.1230e-02,  1.0254e-02, -2.5635e-02,  ...,  3.9062e-03,\n",
       "          -2.1729e-02,  2.5177e-03],\n",
       "         [ 4.8065e-04,  8.8501e-03, -3.1006e-02,  ...,  1.0986e-02,\n",
       "          -4.1504e-02, -8.0566e-03],\n",
       "         ...,\n",
       "         [ 4.1992e-02,  9.6191e-02, -5.5664e-02,  ..., -1.6406e-01,\n",
       "           1.1426e-01,  1.3672e-01],\n",
       "         [ 3.5645e-02,  1.1865e-01,  6.8359e-02,  ..., -2.0605e-01,\n",
       "           1.1865e-01,  1.2695e-01],\n",
       "         [ 2.6953e-01,  2.5781e+00,  9.6484e-01,  ..., -3.0938e+00,\n",
       "          -1.5859e+00,  8.5547e-01]],\n",
       "\n",
       "        [[-2.5330e-03,  5.2490e-03,  1.1047e-02,  ..., -6.4087e-03,\n",
       "          -2.4872e-03, -5.8289e-03],\n",
       "         [-1.0498e-02,  8.1177e-03, -2.9053e-02,  ..., -8.2779e-04,\n",
       "          -1.9409e-02,  2.4033e-04],\n",
       "         [ 3.1891e-03,  2.2278e-03, -3.3936e-02,  ...,  1.2024e-02,\n",
       "          -3.5645e-02, -4.9438e-03],\n",
       "         ...,\n",
       "         [-1.0547e-01, -2.9907e-02, -1.6992e-01,  ..., -1.2402e-01,\n",
       "           6.9824e-02,  1.0156e-01],\n",
       "         [-1.2158e-01, -1.5106e-03, -9.1309e-02,  ..., -6.2988e-02,\n",
       "          -1.7334e-02, -2.6001e-02],\n",
       "         [-2.8906e-01,  2.8438e+00,  3.9453e-01,  ..., -1.9297e+00,\n",
       "          -3.2969e+00, -8.6914e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.5695e-04,  3.0060e-03,  1.2817e-02,  ..., -4.0894e-03,\n",
       "          -3.4790e-03, -6.3171e-03],\n",
       "         [-8.6670e-03,  8.7280e-03, -2.9541e-02,  ...,  5.7983e-04,\n",
       "          -1.6113e-02,  2.5177e-03],\n",
       "         [-6.3705e-04,  6.6223e-03, -4.0283e-02,  ...,  1.1841e-02,\n",
       "          -2.3193e-02, -3.2501e-03],\n",
       "         ...,\n",
       "         [-1.1914e-01,  5.8838e-02, -1.1133e-01,  ..., -1.3672e-01,\n",
       "           9.6191e-02,  1.2500e-01],\n",
       "         [-1.4160e-01,  6.3965e-02,  2.2736e-03,  ..., -9.0332e-02,\n",
       "           5.9570e-02,  5.3711e-02],\n",
       "         [-2.0703e-01,  2.3281e+00,  5.5469e-01,  ..., -2.0781e+00,\n",
       "          -2.1875e+00,  9.2969e-01]],\n",
       "\n",
       "        [[-1.1978e-03,  3.7689e-03,  1.3855e-02,  ..., -1.0071e-03,\n",
       "          -4.9438e-03, -3.2501e-03],\n",
       "         [-8.8501e-03,  8.7280e-03, -1.4893e-02,  ...,  5.0049e-03,\n",
       "          -1.9775e-02,  5.0659e-03],\n",
       "         [ 3.6926e-03,  9.1553e-03, -1.5869e-02,  ...,  7.2327e-03,\n",
       "          -2.1240e-02,  2.9602e-03],\n",
       "         ...,\n",
       "         [ 4.5471e-03,  1.0889e-01,  3.2227e-02,  ...,  3.6621e-02,\n",
       "           2.9175e-02,  2.2070e-01],\n",
       "         [ 5.2246e-02,  2.0215e-01,  1.2891e-01,  ...,  6.6406e-02,\n",
       "          -2.4658e-02,  9.8633e-02],\n",
       "         [ 4.2480e-02,  3.3438e+00,  8.5938e-01,  ..., -1.7266e+00,\n",
       "          -2.3438e+00,  1.2422e+00]],\n",
       "\n",
       "        [[-4.1199e-03,  6.6223e-03,  8.1177e-03,  ..., -2.1362e-03,\n",
       "          -3.1128e-03, -3.9062e-03],\n",
       "         [-1.3245e-02,  8.3618e-03, -1.3428e-02,  ...,  6.3782e-03,\n",
       "          -1.9409e-02,  5.6152e-03],\n",
       "         [-3.4180e-03,  3.9368e-03, -1.8188e-02,  ...,  2.2583e-02,\n",
       "          -3.4180e-02, -5.3101e-03],\n",
       "         ...,\n",
       "         [-1.0400e-01,  7.5195e-02, -1.7456e-02,  ..., -2.3242e-01,\n",
       "          -1.9043e-02,  9.4238e-02],\n",
       "         [-1.5723e-01,  9.9609e-02,  8.9355e-02,  ..., -2.0996e-01,\n",
       "          -9.4238e-02,  1.1841e-02],\n",
       "         [-1.4941e-01,  2.7969e+00,  7.5000e-01,  ..., -3.2188e+00,\n",
       "          -2.9062e+00,  3.3203e-02]]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 10, 452, 2048])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_layer_token_embeddings.permute(1,0,2,3) * attention_mask.unsqueeze(-1)).mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings = ((torch.stack(model_out.hidden_states)[:, torch.arange(input_ids.size(0))] * attention_mask.unsqueeze(-1)).sum(2) / attention_mask.sum(-1).unsqueeze(-1)).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
