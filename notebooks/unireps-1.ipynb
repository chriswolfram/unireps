{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/Users/christopher/Documents/unirepsCache'\n",
    "# cache_dir = '/net/scratch2/chriswolfram/hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'openai-community/gpt2'\n",
    "# model_name = 'meta-llama/Llama-3.2-1B'\n",
    "# model_name = 'google/gemma-2-27b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6803dd947a845b3a7d018e44ff59d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d9e3c9f71744859c08670e6c82cd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5243c95c334129b92de928ed7b2f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e8ed79e1aa437b92a55626984bb1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b037b1fd1d428984e6ea25bc5f1b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43e16c2bec547059435c18b109f604d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82df75631f234d4095ea2cdc0e0e2c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map='auto', cache_dir=cache_dir)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use split slicing and maybe shuffling+streaming to extract samples from large datasets\n",
    "dataset = datasets.load_dataset('stanfordnlp/imdb', cache_dir=cache_dir)\n",
    "dataset = dataset['test'].take(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use split slicing and maybe shuffling+streaming to extract samples from large datasets\n",
    "dataset = datasets.load_dataset('Skylion007/openwebtext', split='train', streaming=True, cache_dir=cache_dir, trust_remote_code=False)\n",
    "# dataset = datasets.load_dataset('EleutherAI/the_pile_deduplicated', split='train', streaming=True, cache_dir=cache_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\\n\\nThe decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\\n\\nCNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the Belgian First Aid and Support Team to evacuate. However, Belgian Chief Coordinator Geert Gijs, a doctor who was at the hospital with 60 Belgian medical personnel, said it was his decision to pull the team out for the night. Gijs said he requested U.N. security personnel to staff the hospital overnight, but was told that peacekeepers would only be able to evacuate the team.\\n\\nHe said it was a \"tough decision\" but that he accepted the U.N. offer to evacuate after a Canadian medical team, also at the hospital with Canadian security officers, left the site Friday afternoon. The Belgian team returned Saturday morning.\\n\\nGijs said the United Nations has agreed to provide security for Saturday night. The team has requested the Belgian government to send its own troops for the field hospital, which Gijs expects to arrive late Sunday.\\n\\nResponding to the CNN report that Gupta was the only doctor left at the Port-au-Prince field hospital, U.N. spokesman Martin Nesirky said Saturday that the world body\\'s mission in Haiti did not order any medical team to leave. If the team left, it was at the request of their own organization, he said.\\n\\nEdmond Mulet, the U.N. assistant secretary general for peacekeeping operations, told reporters later that local security officers deemed the makeshift hospital unsafe.\\n\\n\"It seems that we\\'ve heard some reports in the international media that the United Nations asked or forced some medical teams to not work any more in some clinic -- that is not true, that is completely untrue,\" Mulet said Saturday.\\n\\nCNN video from the scene Friday night shows the Belgian team packing up its supplies and leaving with an escort of blue-helmeted U.N. peacekeepers in marked trucks.\\n\\nView or add to CNN\\'s database of missing persons in Haiti\\n\\nGupta -- assisted by other CNN staffers, security personnel and at least one Haitian nurse who refused to leave -- assessed the needs of the 25 patients, but there was little they could do without supplies.\\n\\nMore people, some in critical condition, were trickling in late Friday.\\n\\n\"I\\'ve never been in a situation like this. This is quite ridiculous,\" Gupta said.\\n\\nWith a dearth of medical facilities in Haiti\\'s capital, ambulances had nowhere else to take patients, some of whom had suffered severe trauma -- amputations and head injuries -- under the rubble. Others had suffered a great deal of blood loss, but there were no blood supplies left at the clinic.\\n\\nGupta feared that some would not survive the night.\\n\\nHe and the others stayed with the injured all night, after the medical team had left and after the generators gave out and the tents turned pitch black.\\n\\nGupta monitored patients\\' vital signs, administered painkillers and continued intravenous drips. He stabilized three new patients in critical condition.\\n\\nAt 3:45 a.m., he posted a message on Twitter: \"pulling all nighter at haiti field hosp. lots of work, but all patients stable. turned my crew into a crack med team tonight.\"\\n\\nAre you in Haiti and safe? Share your photos\\n\\nHe said the Belgian doctors did not want to leave their patients behind but were ordered out by the United Nations, which sent buses to transport them.\\n\\n\"There is concern about riots not far from here -- and this is part of the problem,\" Gupta said.\\n\\nThere have been scattered reports of violence throughout the capital.\\n\\n\"What is striking to me as a physician is that patients who just had surgery, patients who are critically ill, are essentially being left here, nobody to care for them,\" Gupta said.\\n\\nSandra Pierre, a Haitian who has been helping at the makeshift hospital, said the medical staff took most of the supplies with them.\\n\\n\"All the doctors, all the nurses are gone,\" she said. \"They are expected to be back tomorrow. They had no plan on leaving tonight. It was an order that came suddenly.\"\\n\\nShe told Gupta, \"It\\'s just you.\"\\n\\nA 7.0 magnitude earthquake flattened Haiti\\'s capital city Tuesday afternoon, affecting as many as 3 million people as it fanned out across the island nation. Tens of thousands of people are feared dead.\\n\\nHaiti, the poorest nation in the Western hemisphere, lacked adequate medical resources even before the disaster and has been struggling this week to tend to huge numbers of injured. The clinic, set up under several tents, was a godsend to the few who were lucky to have been brought there.\\n\\nRetired Army Lt. Gen. Russel Honore, who led relief efforts for Hurricane Katrina in 2005, said the evacuation of the clinic\\'s medical staff was unforgivable.\\n\\n\"Search and rescue must trump security,\" Honoré said. \"I\\'ve never seen anything like this before in my life. They need to man up and get back in there.\"\\n\\nHonoré drew parallels between the tragedy in New Orleans, Louisiana, and in Port-au-Prince. But even in the chaos of Katrina, he said, he had never seen medical staff walk away.\\n\\n\"I find this astonishing these doctors left,\" he said. \"People are scared of the poor.\"\\n\\nCNN\\'s Justine Redman, Danielle Dellorto and John Bonifield contributed to this report.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.Dataset.from_list(list(dataset.take(1000)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit confusing.\\n\\nThere’s a lot I’d like to talk about. I’ll go through every topic, insted of making the typical what went right/wrong list.\\n\\nConcept\\n\\nWorking over the theme was probably one of the hardest tasks I had to face.\\n\\nOriginally, I had an idea of what kind of game I wanted to develop, gameplay wise – something with lots of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident I could fit any theme around it.\\n\\nIn the end, the problem with a theme like “Evolution” in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game?\\n\\nIn a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it’s not evolution anymore – it’s the equivalent of intelligent design, the fable invented by creationists to combat the very idea of evolution. Being agnostic and a Pastafarian, that’s not something that rubbed me the right way.\\n\\nHence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn’t want to create an “intelligent design” simulator and wrongly call it evolution.\\n\\nThis is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I’d say the only real solution was through the use of artificial selection, somehow. So far, I haven’t seen any entry using this at its core gameplay.\\n\\nAlas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out.\\n\\nMy initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn’t think of compelling (read: serious) mechanics for that.\\n\\nBorgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg?\\n\\nThe third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it.\\n\\nConversations with my inspiring co-worker Roushey (who also created the “Mechanical Underdogs” signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist – by evolving from a normal dinner table.\\n\\nSo the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your “base”. There are 5 other guests at the table, each with their own plate.\\n\\nYour plate can spawn little pieces of pasta. You do so by “ordering” them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying costs, which are debited from your credits (you start with a number of credits).\\n\\nOnce spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps).\\n\\nYour pasta doesn’t like other people’s pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill.\\n\\nOnce a pasta is in the vicinity of a plate, it starts conquering it for its team. It takes around 10 seconds for a plate to be conquered; less if more pasta from the same team are around. If pasta from other team are around, though, they get locked down in their attempt, unable to conquer the plate, until one of them die (think Battlefield’s standard “Conquest” mode).\\n\\nYou get points every second for every plate you own.\\n\\nOver time, the concept also evolved to use an Italian bistro as its main scenario.\\n\\nCarlos, Carlos’ Bistro’s founder and owner\\n\\nSetup\\n\\nNo major changes were made from my work setup. I used FDT and Starling creating an Adobe AIR (ActionScript) project, all tools or frameworks I already had some knowledge with.\\n\\nOne big change for me was that I livestreamed my work through a twitch.tv account. This was a new thing for me. As recommended by Roushey, I used a program called XSplit and I got to say, it is pretty amazing. It made the livestream pretty effortless and the features are awesome, even for the free version. It was great to have some of my friends watch me, and then interact with them and random people through chat. It was also good knowing that I was also recording a local version of the files, so I could make a timelapse video later.\\n\\nKnowing the video was being recorded also made me a lot more self-conscious about my computer use, as if someone was watching over my shoulder. It made me realize that sometimes I spend too much time in seemingly inane tasks (I ended up wasting the longest time just to get some text alignment the way I wanted – it’ll probably drive someone crazy if they watch it) and that I do way too many typos where writing code. I pretty much spend half of the time writing a line and the other half fixing the crazy characters in it.\\n\\nMy own stream was probably boring to watch since I was coding for the most time. But livestreaming is one of the cool things to do as a spectator too. It was great seeing other people working – I had a few tabs opened on my second monitor all the time. It’s actually a bit sad, because if I could, I could have spent the whole weekend just watching other people working! But I had to do my own work, so I’d only do it once in a while, when resting for a bit.\\n\\nDesign\\n\\nAlthough I wanted some simple, low-fi, high-contrast kind of design, I ended up going with somewhat realistic (vector) art. I think it worked very well, fitting the mood of the game, but I also went overboard.\\n\\nFor example: to know the state of a plate (who owns it, who’s conquering it and how much time they have left before conquering it, which pasta units are in the queue, etc), you have to look at the plate’s bill.\\n\\nThe problem I realized when doing some tests is that people never look at the bill! They think it’s some kind of prop, so they never actually read its details.\\n\\nPlus, if you’re zoomed out too much, you can’t actually read it, so it’s hard to know what’s going on with the game until you zoom in to the area of a specific plate.\\n\\nOne other solution that didn’t turn out to be as perfect as I thought was how to indicate who a plate base belongs to. In the game, that’s indicated by the plate’s decoration – its color denotes the team owner. But it’s something that fits so well into the design that people never realized it, until they were told about it.\\n\\nIn the end, the idea of going with a full physical metaphor is one that should be done with care. Things that are very important risk becoming background noise, unless the player knows its importance.\\n\\nOriginally, I wanted to avoid any kind of heads-up display in my game. In the end, I ended up adding it at the bottom to indicate your credits and bases owned, as well as the hideous out-of-place-and-still-not-obvious “Call Waiter” button. But in hindsight, I should have gone with a simple HUD from the start, especially one that indicated each team’s colors and general state of the game without the need for zooming in and out.\\n\\nDevelopment\\n\\nDevelopment went fast. But not fast enough.\\n\\nEven though I worked around 32+ hours for this Ludum Dare, the biggest problem I had to face in the end was overscoping. I had too much planned, and couldn’t get it all done.\\n\\nContent-wise, I had several kinds of pasta planned (Wikipedia is just amazing in that regard), split into several different groups, from small Pastina to huge Pasta al forno. But because of time constraints, I ended up scratching most of them, and ended up with 5 different types of very small pasta – barely something to start when talking about the evolution of Pasta.\\n\\nPastas used in the game. Unfortunately, the macs where never used\\n\\nWhich is one of the saddest things about the project, really. It had the framework and the features to allow an endless number of elements in there, but I just didn’t have time to draw the rest of the assets needed (something I loved to do, by the way).\\n\\nOther non-obvious features had to be dropped, too. For example, when ordering some pasta, you were supposed to select what kind of sauce you’d like with your pasta, each with different attributes. Bolognese, for example, is very strong, but inaccurate; Pesto is very accurate and has great range, but it’s weaker; and my favorite, Vodka, would triggers 10% loss of speed on the pasta hit by it.\\n\\nThe code for that is mostly in there. But in the end, I didn’t have time to implement the sauce selection interface; all pasta ended up using bolognese sauce.\\n\\nTo-do list: lots of things were not done\\n\\nActual programming also took a toll in the development time. Having been programming for a while, I like to believe I got to a point where I know how to make things right, but at the expense of forgetting how to do things wrong in a seemingly good way. What I mean is that I had to take a lot of shortcuts in my code to save time (e.g. a lot of singletons references for cross-communication rather than events or observers, all-encompassing check loops, not fast enough) that left a very sour taste in my mouth. While I know I used to do those a few years ago and survive, I almost cannot accept the state my code is in right now.\\n\\nAt the same time, I do know it was the right thing to do given the timeframe.\\n\\nOne small thing that had some impact was using a somewhat new platform for me. That’s Starling, the accelerated graphics framework I used in Flash. I had tested it before and I knew how to use it well – the API is very similar to Flash itself. However, there were some small details that had some impact during development, making me feel somewhat uneasy the whole time I was writing the game. It was, again, the right thing to do, but I should have used Starling more deeply before (which is the conundrum: I used it for Ludum Dare just so I could learn more about it).\\n\\nArgument and user experience\\n\\nOne final aspect of the game that I learned is that making the game obvious for your players goes a long way into making it fun. If you have to spend the longest time explaining things, your game is doing something wrong.\\n\\nAnd that’s exactly the problem Survival of the Tastiest ultimately faced. It’s very hard for people to understand what’s going on with the game, why, and how. I did have some introductory text at the beginning, but that was a last-minute thing. More importantly, I should have had a better interface or simplified the whole concept so it would be easier for people to understand.\\n\\nThat doesn’t mean the game itself should be simple. It just means that the experience and interface should be approachable and understandable.\\n\\nConclusion\\n\\nI’m extremely happy with what I’ve done and, especially given that this was my first Ludum Dare. However, I feel like I’ve learned a lot of what not to do.\\n\\nThe biggest problem is overscoping. Like Eric Decker said, the biggest lesson we can learn with this is probably with scoping – deciding what to do beforehand in a way you can complete it without having to rush and do something half-assed.\\n\\nI’m sure I will do more Ludum Dares in the future. But if there are any lessons I can take of it, they are to make it simple, to use frameworks and platforms you already have some absolute experience with (otherwise you’ll spend too much time trying to solve easy questions), and to scope for a game that you can complete in one day only (that way, you can actually take two days and make it cool).\\n\\nThis entry was posted\\non Monday, August 27th, 2012 at 10:54 am and is filed under LD #24.\\nYou can follow any responses to this entry through the RSS 2.0 feed.\\nYou can skip to the end and leave a response. Pinging is currently not allowed.\\n\\n3 Responses to ““Survival of the Tastiest” Post-mortem”\\n\\ndarn it , knowing that I missed your livestream makes me a sad panda ;( but more to the point, the game is … well for a startup its original to say the least ;D it has some really neat ideas and more importantly its designed arround touch screens whitch by the looks of the submission is something rare ;o or that could be just me and my short memory -_-! awesum game, love et <3'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0045,  0.0166,  0.0210,  ..., -0.0054, -0.0422, -0.0315],\n",
       "        [ 0.0215, -0.0238,  0.0211,  ..., -0.0107, -0.0011, -0.0374],\n",
       "        [ 0.0136,  0.0104,  0.0128,  ...,  0.0081, -0.0122,  0.0051],\n",
       "        ...,\n",
       "        [ 0.0009,  0.0164, -0.0193,  ..., -0.0003, -0.0030,  0.0066],\n",
       "        [ 0.0009,  0.0164, -0.0193,  ..., -0.0003, -0.0030,  0.0066],\n",
       "        [ 0.0009,  0.0164, -0.0193,  ..., -0.0003, -0.0030,  0.0066]],\n",
       "       device='mps:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d45cd4e17c45c8b280c90f8bbda062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(example):\n",
    "    tokens = tokenizer(example['text'], truncation=True, return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    # The hidden states are cast to float32 becauase it avoids some numerical issues, and everything\n",
    "    # will have to be cast eventually because Arrow doesn't currently support types like bfloat16.\n",
    "    layer_token_embeddings = torch.stack(model_out.hidden_states)[:,0].float()\n",
    "\n",
    "    # Outputs are moved to the CPU to avoid memory problems on the GPU.\n",
    "    layer_last_embeddings = layer_token_embeddings[:,-1].cpu()\n",
    "    layer_mean_embeddings = layer_token_embeddings.mean(1).cpu()\n",
    "\n",
    "    return {\n",
    "        'layer_last_embeddings': layer_last_embeddings,\n",
    "        'layer_mean_embeddings': layer_mean_embeddings\n",
    "    }\n",
    "\n",
    "# TODO: This currently sets new_fingerprint because otherwise `map` appears to hash compute_embeddings which includes the entire model!\n",
    "# This does not use batching. In experiments, batching (somehow) slightly slowed it down across models and hardware!\n",
    "embeddings = dataset.take(128).map(compute_embeddings, new_fingerprint='test_fingerprint')\n",
    "embeddings.set_format('torch')\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1699e-02, -2.0490e-03,  7.3117e-03,  ..., -6.9872e-02,\n",
       "          4.8656e-02,  9.6262e-02],\n",
       "        [ 5.7095e-01, -1.3241e-01, -2.1094e-02,  ..., -8.0342e-01,\n",
       "         -2.6857e-01, -8.2984e-02],\n",
       "        [ 4.3036e-01, -1.5056e-01, -9.2372e-02,  ..., -9.6341e-01,\n",
       "         -1.0211e-01,  1.9029e-01],\n",
       "        ...,\n",
       "        [ 8.2161e-01, -4.0093e+00,  3.6737e-01,  ..., -8.4385e-01,\n",
       "         -9.6017e-01,  5.1732e+00],\n",
       "        [ 1.6561e+00, -2.9995e+00, -6.4317e-01,  ...,  7.8280e-01,\n",
       "         -3.0466e+00,  8.2880e+00],\n",
       "        [ 8.4603e-02, -1.4935e-01,  1.8391e-02,  ...,  7.9412e-02,\n",
       "         -2.6802e-01,  7.1571e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[-1]['layer_last_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0482e-01,  4.1662e-01, -1.7881e-02,  ...,  1.4470e-01,\n",
       "          1.9332e-01, -5.1653e-02],\n",
       "        [-1.2791e-01,  8.6794e-02,  1.1645e-02,  ...,  2.4013e-02,\n",
       "          6.9477e-02, -1.4329e-01],\n",
       "        [ 5.5292e-02, -1.2492e-01, -1.9515e-02,  ...,  7.1906e-03,\n",
       "          4.3979e-02, -4.8419e-02],\n",
       "        ...,\n",
       "        [ 8.2082e+01,  6.4029e+01,  1.7222e+00,  ...,  2.3686e+01,\n",
       "         -8.0630e+00, -7.2619e+01],\n",
       "        [ 1.2726e+02,  8.3658e+01, -4.8099e+01,  ..., -3.4301e+00,\n",
       "         -1.3376e+01, -1.1925e+02],\n",
       "        [ 2.4463e+00,  2.1555e+00, -8.4142e-01,  ..., -1.2124e+00,\n",
       "         -1.1269e+00, -6.6522e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[-1]['layer_mean_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=tokenizer(embeddings['text'][-1], return_tensors='pt').input_ids.to(model.device), output_hidden_states=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3045e-01,  5.2381e-01,  8.2463e-01,  ..., -1.1306e-01,\n",
       "          3.3893e-02, -2.3120e-02],\n",
       "        [-1.0142e-01,  2.8770e-01,  3.6419e-01,  ...,  6.0856e-02,\n",
       "         -2.9304e-02, -8.4377e-02],\n",
       "        [-3.7873e-02, -3.0839e-02,  3.2535e-01,  ...,  2.4453e-01,\n",
       "         -9.8545e-02,  5.0027e-02],\n",
       "        ...,\n",
       "        [-5.3461e+01, -3.4155e+00, -2.3939e+02,  ...,  2.5938e+02,\n",
       "          1.3480e+02, -1.9434e+02],\n",
       "        [-3.0611e+02, -3.3730e+01, -4.7096e+02,  ...,  1.6534e+02,\n",
       "          1.7137e+02, -4.1980e+02],\n",
       "        [-9.2746e+00,  6.5281e-02, -1.0728e+01,  ...,  5.8639e+00,\n",
       "          2.6707e+00, -1.5843e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0482e-01,  4.1662e-01, -1.7881e-02,  ...,  1.4470e-01,\n",
       "          1.9332e-01, -5.1653e-02],\n",
       "        [-1.2791e-01,  8.6794e-02,  1.1645e-02,  ...,  2.4013e-02,\n",
       "          6.9477e-02, -1.4329e-01],\n",
       "        [ 5.5292e-02, -1.2492e-01, -1.9515e-02,  ...,  7.1906e-03,\n",
       "          4.3979e-02, -4.8419e-02],\n",
       "        ...,\n",
       "        [ 8.2082e+01,  6.4029e+01,  1.7222e+00,  ...,  2.3686e+01,\n",
       "         -8.0630e+00, -7.2619e+01],\n",
       "        [ 1.2726e+02,  8.3658e+01, -4.8099e+01,  ..., -3.4301e+00,\n",
       "         -1.3376e+01, -1.1925e+02],\n",
       "        [ 2.4463e+00,  2.1555e+00, -8.4142e-01,  ..., -1.2124e+00,\n",
       "         -1.1269e+00, -6.6522e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].float().mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5535,  1.9119,  0.9343,  ...,  1.7148, -1.6708,  1.1468],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).mean(2)[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5547, -1.5234,  2.7812,  ..., -0.6445,  0.6094, -1.0469],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings[0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 98, 2048])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_output.hidden_states).permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00, -6.1035e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5259e-05,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.0518e-05,  1.5259e-05,  2.4414e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.0518e-05],\n",
       "        ...,\n",
       "        [-9.7656e-04,  0.0000e+00,  4.8828e-04,  ..., -9.7656e-04,\n",
       "         -4.8828e-04,  0.0000e+00],\n",
       "        [ 0.0000e+00,  9.7656e-04, -1.2207e-04,  ...,  1.9531e-03,\n",
       "         -1.2207e-03,  8.5449e-04],\n",
       "        [-3.9062e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.9062e-03]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings['layer_mean_embeddings'][-1] - torch.stack(model_output.hidden_states)[:,0].mean(1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 161, 2048])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1414e-02, -2.2278e-03, -1.1292e-03,  ..., -1.7944e-02,\n",
       "         -2.6855e-03, -2.1729e-02],\n",
       "        [ 2.0142e-03,  4.8828e-03, -1.0400e-01,  ..., -2.1729e-02,\n",
       "         -2.8809e-02, -3.7354e-02],\n",
       "        [ 7.7515e-03, -5.6152e-02, -1.5430e-01,  ...,  5.4932e-02,\n",
       "         -3.9551e-02, -6.4941e-02],\n",
       "        ...,\n",
       "        [-9.5703e-02,  3.2227e-01, -5.9766e-01,  ..., -7.6562e-01,\n",
       "         -7.1289e-02, -5.2246e-02],\n",
       "        [ 1.9141e-01,  4.3359e-01, -5.6641e-01,  ..., -6.3281e-01,\n",
       "         -9.3262e-02, -6.0547e-02],\n",
       "        [ 2.5312e+00,  3.9062e+00,  2.3730e-01,  ..., -4.7188e+00,\n",
       "         -4.8438e+00, -1.0781e+00]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9182e-04,  3.4027e-03,  1.2512e-02,  ..., -8.8501e-04,\n",
       "         -6.8970e-03, -5.7068e-03],\n",
       "        [-1.2085e-02,  8.4229e-03, -1.9653e-02,  ...,  9.2773e-03,\n",
       "         -2.4048e-02,  4.2419e-03],\n",
       "        [ 1.1292e-03,  3.9673e-03, -2.4658e-02,  ...,  1.7700e-02,\n",
       "         -3.5400e-02, -3.6926e-03],\n",
       "        ...,\n",
       "        [ 3.4668e-02,  1.6309e-01, -1.3184e-01,  ..., -9.6680e-02,\n",
       "         -1.6724e-02,  7.3730e-02],\n",
       "        [ 9.0332e-02,  2.1484e-01, -7.2754e-02,  ..., -7.2754e-02,\n",
       "         -1.3281e-01, -2.6611e-02],\n",
       "        [ 1.3125e+00,  3.5312e+00,  6.1719e-01,  ..., -2.7969e+00,\n",
       "         -3.2812e+00, -1.4648e-01]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'model_out': model_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_model_out(dataset.take(10))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = out['input_ids'].cpu()\n",
    "attention_mask = out['attention_mask'].cpu()\n",
    "model_out = out['model_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_indices = attention_mask.sum(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_out.hidden_states).cpu().permute(1,0,2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_out.hidden_states).permute(1,0,2,3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_last_embedding = input_layer_token_embeddings[torch.arange(input_layer_token_embeddings.shape[0]), :, last_token_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 452])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 452])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 452, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([315, 279, 146, 452, 160, 218, 361, 208, 177, 216])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_mean_embeddings = (input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9073e-03,  3.4180e-03,  1.1902e-02,  ..., -5.9814e-03,\n",
       "          -5.7068e-03, -4.3640e-03],\n",
       "         [-1.2329e-02,  1.1475e-02, -1.5625e-02,  ...,  4.3640e-03,\n",
       "          -1.2085e-02,  1.6098e-03],\n",
       "         [ 1.9531e-03,  6.3782e-03, -1.8555e-02,  ...,  1.0010e-02,\n",
       "          -1.8433e-02, -5.5847e-03],\n",
       "         ...,\n",
       "         [-1.3086e-01, -3.5156e-02, -4.4861e-03,  ..., -1.4258e-01,\n",
       "           3.2959e-02,  9.8633e-02],\n",
       "         [-1.3965e-01,  3.3691e-02,  1.0840e-01,  ..., -1.3867e-01,\n",
       "           2.4261e-03,  1.0107e-01],\n",
       "         [-4.1406e-01,  2.4688e+00,  1.1172e+00,  ..., -2.7031e+00,\n",
       "          -2.7656e+00,  7.1094e-01]],\n",
       "\n",
       "        [[-2.7924e-03,  2.7618e-03,  1.0254e-02,  ..., -3.5400e-03,\n",
       "          -4.1809e-03, -3.6621e-03],\n",
       "         [-1.1230e-02,  1.0254e-02, -2.5635e-02,  ...,  3.9062e-03,\n",
       "          -2.1729e-02,  2.5177e-03],\n",
       "         [ 4.8065e-04,  8.8501e-03, -3.1006e-02,  ...,  1.0986e-02,\n",
       "          -4.1504e-02, -8.0566e-03],\n",
       "         ...,\n",
       "         [ 4.1992e-02,  9.6191e-02, -5.5664e-02,  ..., -1.6406e-01,\n",
       "           1.1426e-01,  1.3672e-01],\n",
       "         [ 3.5645e-02,  1.1865e-01,  6.8359e-02,  ..., -2.0605e-01,\n",
       "           1.1865e-01,  1.2695e-01],\n",
       "         [ 2.6953e-01,  2.5781e+00,  9.6484e-01,  ..., -3.0938e+00,\n",
       "          -1.5859e+00,  8.5547e-01]],\n",
       "\n",
       "        [[-2.5330e-03,  5.2490e-03,  1.1047e-02,  ..., -6.4087e-03,\n",
       "          -2.4872e-03, -5.8289e-03],\n",
       "         [-1.0498e-02,  8.1177e-03, -2.9053e-02,  ..., -8.2779e-04,\n",
       "          -1.9409e-02,  2.4033e-04],\n",
       "         [ 3.1891e-03,  2.2278e-03, -3.3936e-02,  ...,  1.2024e-02,\n",
       "          -3.5645e-02, -4.9438e-03],\n",
       "         ...,\n",
       "         [-1.0547e-01, -2.9907e-02, -1.6992e-01,  ..., -1.2402e-01,\n",
       "           6.9824e-02,  1.0156e-01],\n",
       "         [-1.2158e-01, -1.5106e-03, -9.1309e-02,  ..., -6.2988e-02,\n",
       "          -1.7334e-02, -2.6001e-02],\n",
       "         [-2.8906e-01,  2.8438e+00,  3.9453e-01,  ..., -1.9297e+00,\n",
       "          -3.2969e+00, -8.6914e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.5695e-04,  3.0060e-03,  1.2817e-02,  ..., -4.0894e-03,\n",
       "          -3.4790e-03, -6.3171e-03],\n",
       "         [-8.6670e-03,  8.7280e-03, -2.9541e-02,  ...,  5.7983e-04,\n",
       "          -1.6113e-02,  2.5177e-03],\n",
       "         [-6.3705e-04,  6.6223e-03, -4.0283e-02,  ...,  1.1841e-02,\n",
       "          -2.3193e-02, -3.2501e-03],\n",
       "         ...,\n",
       "         [-1.1914e-01,  5.8838e-02, -1.1133e-01,  ..., -1.3672e-01,\n",
       "           9.6191e-02,  1.2500e-01],\n",
       "         [-1.4160e-01,  6.3965e-02,  2.2736e-03,  ..., -9.0332e-02,\n",
       "           5.9570e-02,  5.3711e-02],\n",
       "         [-2.0703e-01,  2.3281e+00,  5.5469e-01,  ..., -2.0781e+00,\n",
       "          -2.1875e+00,  9.2969e-01]],\n",
       "\n",
       "        [[-1.1978e-03,  3.7689e-03,  1.3855e-02,  ..., -1.0071e-03,\n",
       "          -4.9438e-03, -3.2501e-03],\n",
       "         [-8.8501e-03,  8.7280e-03, -1.4893e-02,  ...,  5.0049e-03,\n",
       "          -1.9775e-02,  5.0659e-03],\n",
       "         [ 3.6926e-03,  9.1553e-03, -1.5869e-02,  ...,  7.2327e-03,\n",
       "          -2.1240e-02,  2.9602e-03],\n",
       "         ...,\n",
       "         [ 4.5471e-03,  1.0889e-01,  3.2227e-02,  ...,  3.6621e-02,\n",
       "           2.9175e-02,  2.2070e-01],\n",
       "         [ 5.2246e-02,  2.0215e-01,  1.2891e-01,  ...,  6.6406e-02,\n",
       "          -2.4658e-02,  9.8633e-02],\n",
       "         [ 4.2480e-02,  3.3438e+00,  8.5938e-01,  ..., -1.7266e+00,\n",
       "          -2.3438e+00,  1.2422e+00]],\n",
       "\n",
       "        [[-4.1199e-03,  6.6223e-03,  8.1177e-03,  ..., -2.1362e-03,\n",
       "          -3.1128e-03, -3.9062e-03],\n",
       "         [-1.3245e-02,  8.3618e-03, -1.3428e-02,  ...,  6.3782e-03,\n",
       "          -1.9409e-02,  5.6152e-03],\n",
       "         [-3.4180e-03,  3.9368e-03, -1.8188e-02,  ...,  2.2583e-02,\n",
       "          -3.4180e-02, -5.3101e-03],\n",
       "         ...,\n",
       "         [-1.0400e-01,  7.5195e-02, -1.7456e-02,  ..., -2.3242e-01,\n",
       "          -1.9043e-02,  9.4238e-02],\n",
       "         [-1.5723e-01,  9.9609e-02,  8.9355e-02,  ..., -2.0996e-01,\n",
       "          -9.4238e-02,  1.1841e-02],\n",
       "         [-1.4941e-01,  2.7969e+00,  7.5000e-01,  ..., -3.2188e+00,\n",
       "          -2.9062e+00,  3.3203e-02]]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 10, 452, 2048])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_layer_token_embeddings.permute(1,0,2,3) * attention_mask.unsqueeze(-1)).mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings = ((torch.stack(model_out.hidden_states)[:, torch.arange(input_ids.size(0))] * attention_mask.unsqueeze(-1)).sum(2) / attention_mask.sum(-1).unsqueeze(-1)).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
