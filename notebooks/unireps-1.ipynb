{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/Users/christopher/Documents/unirepsCache'\n",
    "# cache_dir = '/net/scratch2/chriswolfram/hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map='auto', cache_dir=cache_dir)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use split slicing and maybe shuffling+streaming to extract samples from large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_datadataset('stanfordnlp/imdb', cache_dir=cache_dir)\n",
    "dataset = dataset['test'].take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0045,  0.0166,  0.0210,  ..., -0.0054, -0.0422, -0.0315],\n",
       "        [ 0.0215, -0.0238,  0.0211,  ..., -0.0107, -0.0011, -0.0374],\n",
       "        [ 0.0136,  0.0104,  0.0128,  ...,  0.0081, -0.0122,  0.0051],\n",
       "        ...,\n",
       "        [ 0.0009,  0.0164, -0.0193,  ..., -0.0003, -0.0030,  0.0066],\n",
       "        [ 0.0009,  0.0164, -0.0193,  ..., -0.0003, -0.0030,  0.0066],\n",
       "        [ 0.0009,  0.0164, -0.0193,  ..., -0.0003, -0.0030,  0.0066]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664d89a174141f9aadd0f21a26c1543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    last_token_indices = attention_mask.sum(dim=-1) - 1\n",
    "    embeddings = model_out.hidden_states[-1][torch.arange(input_ids.size(0)), last_token_indices]\n",
    "    \n",
    "    return {'embeddings': embeddings}\n",
    "\n",
    "train_embeddings = dataset.map(compute_embeddings, batched=True, batch_size=64)\n",
    "train_embeddings.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(dataset['text'][0], padding='longest', return_tensors='pt')\n",
    "input_ids = tokens.input_ids.to(model.device)\n",
    "attention_mask = tokens.attention_mask.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9035e-03,  3.4298e-03,  1.1937e-02,  ..., -6.0098e-03,\n",
       "         -5.7379e-03, -4.3657e-03],\n",
       "        [-1.2339e-02,  1.1494e-02, -1.5648e-02,  ...,  4.3412e-03,\n",
       "         -1.2122e-02,  1.6042e-03],\n",
       "        [ 1.9405e-03,  6.4132e-03, -1.8607e-02,  ...,  9.9847e-03,\n",
       "         -1.8577e-02, -5.6446e-03],\n",
       "        ...,\n",
       "        [-1.3252e-01, -3.6015e-02, -4.6515e-03,  ..., -1.4342e-01,\n",
       "          3.2569e-02,  9.9657e-02],\n",
       "        [-1.3976e-01,  3.2463e-02,  1.0859e-01,  ..., -1.3966e-01,\n",
       "          2.4561e-03,  1.0292e-01],\n",
       "        [-4.1964e-01,  2.4690e+00,  1.1171e+00,  ..., -2.7177e+00,\n",
       "         -2.7629e+00,  7.1740e-01]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,:,:].float().mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9035e-03,  3.4298e-03,  1.1937e-02,  ..., -6.0098e-03,\n",
       "         -5.7379e-03, -4.3657e-03],\n",
       "        [-1.2339e-02,  1.1494e-02, -1.5648e-02,  ...,  4.3412e-03,\n",
       "         -1.2122e-02,  1.6042e-03],\n",
       "        [ 1.9405e-03,  6.4132e-03, -1.8607e-02,  ...,  9.9847e-03,\n",
       "         -1.8577e-02, -5.6446e-03],\n",
       "        ...,\n",
       "        [-1.3252e-01, -3.6015e-02, -4.6515e-03,  ..., -1.4342e-01,\n",
       "          3.2569e-02,  9.9657e-02],\n",
       "        [-1.3976e-01,  3.2463e-02,  1.0859e-01,  ..., -1.3966e-01,\n",
       "          2.4561e-03,  1.0292e-01],\n",
       "        [-4.1964e-01,  2.4690e+00,  1.1171e+00,  ..., -2.7177e+00,\n",
       "         -2.7629e+00,  7.1740e-01]], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,:,:].float().sum(1) / (torch.stack(model_output.hidden_states)[:,0,:,:].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just don't use batching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8835f7536cd745f2906d9ffa90d26e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, output_hidden_states=True)\n",
    "\n",
    "    # TODO: Does output_type do anything?\n",
    "    output_type = model_out.hidden_states[0].dtype\n",
    "    \n",
    "    layer_token_embeddings = torch.stack(model_out.hidden_states)[:,0].float()\n",
    "\n",
    "    layer_last_embeddings = layer_token_embeddings[-1]\n",
    "    layer_mean_embeddings = layer_token_embeddings.mean(1)\n",
    "\n",
    "    return {'layer_last_embeddings': layer_last_embeddings.to(output_type), 'layer_mean_embeddings': layer_mean_embeddings.to(output_type)}\n",
    "\n",
    "train_embeddings3 = dataset.map(compute_embeddings)\n",
    "train_embeddings3.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad28f226ff82410a9cd976ab3688a5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "    # TODO: Does output_type do anything?\n",
    "    output_type = model_out.hidden_states[0].dtype\n",
    "    \n",
    "    input_layer_token_embeddings = torch.stack(model_out.hidden_states).permute(1,0,2,3).float()\n",
    "\n",
    "    last_token_indices = attention_mask.sum(-1) - 1\n",
    "    layer_last_embeddings = input_layer_token_embeddings[torch.arange(input_layer_token_embeddings.shape[0]), :, last_token_indices]\n",
    "    layer_mean_embeddings = (input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    return {'layer_last_embeddings': layer_last_embeddings.to(output_type), 'layer_mean_embeddings': layer_mean_embeddings.to(output_type)}\n",
    "\n",
    "train_embeddings2 = dataset.map(compute_embeddings, batched=True, batch_size=1)\n",
    "train_embeddings2.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6445,  3.2969,  0.7656,  ..., -4.8750, -2.5000,  1.7969])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings3['layer_last_embeddings'][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6445,  3.2969,  0.7656,  ..., -4.8750, -2.5000,  1.7969])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings2['layer_last_embeddings'][-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0425,  3.3438,  0.8633,  ..., -1.7344, -2.3438,  1.2422])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings3['layer_mean_embeddings'][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0425,  3.3438,  0.8633,  ..., -1.7344, -2.3438,  1.2422])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings2['layer_mean_embeddings'][-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=tokenizer(train_embeddings3['text'][-1], padding='longest', return_tensors='pt').input_ids.to(model.device), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6445,  3.2969,  0.7656,  ..., -4.8750, -2.5000,  1.7969],\n",
       "       device='mps:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[-1,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 2048])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].mean(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0425,  3.3438,  0.8633,  ..., -1.7344, -2.3438,  1.2422],\n",
       "       device='mps:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).mean(2)[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5547, -1.5234,  2.7812,  ..., -0.6445,  0.6094, -1.0469],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings[0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 98, 2048])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_output.hidden_states).permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00, -6.1035e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5259e-05,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.0518e-05,  1.5259e-05,  2.4414e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.0518e-05],\n",
       "        ...,\n",
       "        [-9.7656e-04,  0.0000e+00,  4.8828e-04,  ..., -9.7656e-04,\n",
       "         -4.8828e-04,  0.0000e+00],\n",
       "        [ 0.0000e+00,  9.7656e-04, -1.2207e-04,  ...,  1.9531e-03,\n",
       "         -1.2207e-03,  8.5449e-04],\n",
       "        [-3.9062e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.9062e-03]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings['layer_mean_embeddings'][-1] - torch.stack(model_output.hidden_states)[:,0].mean(1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 161, 2048])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1414e-02, -2.2278e-03, -1.1292e-03,  ..., -1.7944e-02,\n",
       "         -2.6855e-03, -2.1729e-02],\n",
       "        [ 2.0142e-03,  4.8828e-03, -1.0400e-01,  ..., -2.1729e-02,\n",
       "         -2.8809e-02, -3.7354e-02],\n",
       "        [ 7.7515e-03, -5.6152e-02, -1.5430e-01,  ...,  5.4932e-02,\n",
       "         -3.9551e-02, -6.4941e-02],\n",
       "        ...,\n",
       "        [-9.5703e-02,  3.2227e-01, -5.9766e-01,  ..., -7.6562e-01,\n",
       "         -7.1289e-02, -5.2246e-02],\n",
       "        [ 1.9141e-01,  4.3359e-01, -5.6641e-01,  ..., -6.3281e-01,\n",
       "         -9.3262e-02, -6.0547e-02],\n",
       "        [ 2.5312e+00,  3.9062e+00,  2.3730e-01,  ..., -4.7188e+00,\n",
       "         -4.8438e+00, -1.0781e+00]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9182e-04,  3.4027e-03,  1.2512e-02,  ..., -8.8501e-04,\n",
       "         -6.8970e-03, -5.7068e-03],\n",
       "        [-1.2085e-02,  8.4229e-03, -1.9653e-02,  ...,  9.2773e-03,\n",
       "         -2.4048e-02,  4.2419e-03],\n",
       "        [ 1.1292e-03,  3.9673e-03, -2.4658e-02,  ...,  1.7700e-02,\n",
       "         -3.5400e-02, -3.6926e-03],\n",
       "        ...,\n",
       "        [ 3.4668e-02,  1.6309e-01, -1.3184e-01,  ..., -9.6680e-02,\n",
       "         -1.6724e-02,  7.3730e-02],\n",
       "        [ 9.0332e-02,  2.1484e-01, -7.2754e-02,  ..., -7.2754e-02,\n",
       "         -1.3281e-01, -2.6611e-02],\n",
       "        [ 1.3125e+00,  3.5312e+00,  6.1719e-01,  ..., -2.7969e+00,\n",
       "         -3.2812e+00, -1.4648e-01]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'model_out': model_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_model_out(dataset.take(10))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = out['input_ids'].cpu()\n",
    "attention_mask = out['attention_mask'].cpu()\n",
    "model_out = out['model_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_indices = attention_mask.sum(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_out.hidden_states).cpu().permute(1,0,2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_out.hidden_states).permute(1,0,2,3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_last_embedding = input_layer_token_embeddings[torch.arange(input_layer_token_embeddings.shape[0]), :, last_token_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 452])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 452])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 452, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([315, 279, 146, 452, 160, 218, 361, 208, 177, 216])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_mean_embeddings = (input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9073e-03,  3.4180e-03,  1.1902e-02,  ..., -5.9814e-03,\n",
       "          -5.7068e-03, -4.3640e-03],\n",
       "         [-1.2329e-02,  1.1475e-02, -1.5625e-02,  ...,  4.3640e-03,\n",
       "          -1.2085e-02,  1.6098e-03],\n",
       "         [ 1.9531e-03,  6.3782e-03, -1.8555e-02,  ...,  1.0010e-02,\n",
       "          -1.8433e-02, -5.5847e-03],\n",
       "         ...,\n",
       "         [-1.3086e-01, -3.5156e-02, -4.4861e-03,  ..., -1.4258e-01,\n",
       "           3.2959e-02,  9.8633e-02],\n",
       "         [-1.3965e-01,  3.3691e-02,  1.0840e-01,  ..., -1.3867e-01,\n",
       "           2.4261e-03,  1.0107e-01],\n",
       "         [-4.1406e-01,  2.4688e+00,  1.1172e+00,  ..., -2.7031e+00,\n",
       "          -2.7656e+00,  7.1094e-01]],\n",
       "\n",
       "        [[-2.7924e-03,  2.7618e-03,  1.0254e-02,  ..., -3.5400e-03,\n",
       "          -4.1809e-03, -3.6621e-03],\n",
       "         [-1.1230e-02,  1.0254e-02, -2.5635e-02,  ...,  3.9062e-03,\n",
       "          -2.1729e-02,  2.5177e-03],\n",
       "         [ 4.8065e-04,  8.8501e-03, -3.1006e-02,  ...,  1.0986e-02,\n",
       "          -4.1504e-02, -8.0566e-03],\n",
       "         ...,\n",
       "         [ 4.1992e-02,  9.6191e-02, -5.5664e-02,  ..., -1.6406e-01,\n",
       "           1.1426e-01,  1.3672e-01],\n",
       "         [ 3.5645e-02,  1.1865e-01,  6.8359e-02,  ..., -2.0605e-01,\n",
       "           1.1865e-01,  1.2695e-01],\n",
       "         [ 2.6953e-01,  2.5781e+00,  9.6484e-01,  ..., -3.0938e+00,\n",
       "          -1.5859e+00,  8.5547e-01]],\n",
       "\n",
       "        [[-2.5330e-03,  5.2490e-03,  1.1047e-02,  ..., -6.4087e-03,\n",
       "          -2.4872e-03, -5.8289e-03],\n",
       "         [-1.0498e-02,  8.1177e-03, -2.9053e-02,  ..., -8.2779e-04,\n",
       "          -1.9409e-02,  2.4033e-04],\n",
       "         [ 3.1891e-03,  2.2278e-03, -3.3936e-02,  ...,  1.2024e-02,\n",
       "          -3.5645e-02, -4.9438e-03],\n",
       "         ...,\n",
       "         [-1.0547e-01, -2.9907e-02, -1.6992e-01,  ..., -1.2402e-01,\n",
       "           6.9824e-02,  1.0156e-01],\n",
       "         [-1.2158e-01, -1.5106e-03, -9.1309e-02,  ..., -6.2988e-02,\n",
       "          -1.7334e-02, -2.6001e-02],\n",
       "         [-2.8906e-01,  2.8438e+00,  3.9453e-01,  ..., -1.9297e+00,\n",
       "          -3.2969e+00, -8.6914e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.5695e-04,  3.0060e-03,  1.2817e-02,  ..., -4.0894e-03,\n",
       "          -3.4790e-03, -6.3171e-03],\n",
       "         [-8.6670e-03,  8.7280e-03, -2.9541e-02,  ...,  5.7983e-04,\n",
       "          -1.6113e-02,  2.5177e-03],\n",
       "         [-6.3705e-04,  6.6223e-03, -4.0283e-02,  ...,  1.1841e-02,\n",
       "          -2.3193e-02, -3.2501e-03],\n",
       "         ...,\n",
       "         [-1.1914e-01,  5.8838e-02, -1.1133e-01,  ..., -1.3672e-01,\n",
       "           9.6191e-02,  1.2500e-01],\n",
       "         [-1.4160e-01,  6.3965e-02,  2.2736e-03,  ..., -9.0332e-02,\n",
       "           5.9570e-02,  5.3711e-02],\n",
       "         [-2.0703e-01,  2.3281e+00,  5.5469e-01,  ..., -2.0781e+00,\n",
       "          -2.1875e+00,  9.2969e-01]],\n",
       "\n",
       "        [[-1.1978e-03,  3.7689e-03,  1.3855e-02,  ..., -1.0071e-03,\n",
       "          -4.9438e-03, -3.2501e-03],\n",
       "         [-8.8501e-03,  8.7280e-03, -1.4893e-02,  ...,  5.0049e-03,\n",
       "          -1.9775e-02,  5.0659e-03],\n",
       "         [ 3.6926e-03,  9.1553e-03, -1.5869e-02,  ...,  7.2327e-03,\n",
       "          -2.1240e-02,  2.9602e-03],\n",
       "         ...,\n",
       "         [ 4.5471e-03,  1.0889e-01,  3.2227e-02,  ...,  3.6621e-02,\n",
       "           2.9175e-02,  2.2070e-01],\n",
       "         [ 5.2246e-02,  2.0215e-01,  1.2891e-01,  ...,  6.6406e-02,\n",
       "          -2.4658e-02,  9.8633e-02],\n",
       "         [ 4.2480e-02,  3.3438e+00,  8.5938e-01,  ..., -1.7266e+00,\n",
       "          -2.3438e+00,  1.2422e+00]],\n",
       "\n",
       "        [[-4.1199e-03,  6.6223e-03,  8.1177e-03,  ..., -2.1362e-03,\n",
       "          -3.1128e-03, -3.9062e-03],\n",
       "         [-1.3245e-02,  8.3618e-03, -1.3428e-02,  ...,  6.3782e-03,\n",
       "          -1.9409e-02,  5.6152e-03],\n",
       "         [-3.4180e-03,  3.9368e-03, -1.8188e-02,  ...,  2.2583e-02,\n",
       "          -3.4180e-02, -5.3101e-03],\n",
       "         ...,\n",
       "         [-1.0400e-01,  7.5195e-02, -1.7456e-02,  ..., -2.3242e-01,\n",
       "          -1.9043e-02,  9.4238e-02],\n",
       "         [-1.5723e-01,  9.9609e-02,  8.9355e-02,  ..., -2.0996e-01,\n",
       "          -9.4238e-02,  1.1841e-02],\n",
       "         [-1.4941e-01,  2.7969e+00,  7.5000e-01,  ..., -3.2188e+00,\n",
       "          -2.9062e+00,  3.3203e-02]]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 10, 452, 2048])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_layer_token_embeddings.permute(1,0,2,3) * attention_mask.unsqueeze(-1)).mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings = ((torch.stack(model_out.hidden_states)[:, torch.arange(input_ids.size(0))] * attention_mask.unsqueeze(-1)).sum(2) / attention_mask.sum(-1).unsqueeze(-1)).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
