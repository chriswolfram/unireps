{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_dir = '/Users/christopher/Documents/unirepsCache'\n",
    "cache_dir = '/net/scratch2/chriswolfram/hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'meta-llama/Llama-3.2-1B'\n",
    "model_name = 'google/gemma-2-27b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43aae2fc4c544538ed5591d6fe43f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map='auto', cache_dir=cache_dir)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto', cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use split slicing and maybe shuffling+streaming to extract samples from large datasets\n",
    "dataset = datasets.load_dataset('stanfordnlp/imdb', cache_dir=cache_dir)\n",
    "dataset = dataset['test'].take(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0055, -0.0071,  0.0009,  ...,  0.0044,  0.0036,  0.0037],\n",
       "        [-0.0318,  0.0242,  0.0067,  ...,  0.0007,  0.0008, -0.0130],\n",
       "        [ 0.0048, -0.0054, -0.0048,  ...,  0.0069, -0.0050,  0.0040],\n",
       "        ...,\n",
       "        [-0.0077,  0.0068, -0.0240,  ...,  0.0234,  0.0228, -0.0018],\n",
       "        [ 0.0109,  0.0115, -0.0016,  ..., -0.0034, -0.0011,  0.0138],\n",
       "        [ 0.0058, -0.0071,  0.0007,  ...,  0.0043,  0.0035,  0.0038]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function compute_embeddings at 0x7f4e4a27f400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2a3911c08b43148d67a223e716d230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_embeddings(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    layer_token_embeddings = torch.stack(model_out.hidden_states)[:,0].float()\n",
    "\n",
    "    layer_last_embeddings = layer_token_embeddings[:,-1].cpu()\n",
    "    layer_mean_embeddings = layer_token_embeddings.mean(1).cpu()\n",
    "\n",
    "    return {'layer_last_embeddings': layer_last_embeddings, 'layer_mean_embeddings': layer_mean_embeddings}\n",
    "\n",
    "# TODO: This currently sets new_fingerprint because otherwise `map` appears to hash compute_embeddings which includes the entire model!\n",
    "embeddings = dataset.take(128).map(compute_embeddings, new_fingerprint='test_fingerprint', load_from_cache_file=False)\n",
    "embeddings.set_format('torch')\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3045e-01,  5.2381e-01,  8.2463e-01,  ..., -1.1306e-01,\n",
       "          3.3893e-02, -2.3120e-02],\n",
       "        [-1.0142e-01,  2.8770e-01,  3.6419e-01,  ...,  6.0856e-02,\n",
       "         -2.9304e-02, -8.4377e-02],\n",
       "        [-3.7873e-02, -3.0839e-02,  3.2535e-01,  ...,  2.4453e-01,\n",
       "         -9.8545e-02,  5.0027e-02],\n",
       "        ...,\n",
       "        [-5.3461e+01, -3.4155e+00, -2.3939e+02,  ...,  2.5938e+02,\n",
       "          1.3480e+02, -1.9434e+02],\n",
       "        [-3.0611e+02, -3.3730e+01, -4.7096e+02,  ...,  1.6534e+02,\n",
       "          1.7137e+02, -4.1980e+02],\n",
       "        [-9.2746e+00,  6.5281e-02, -1.0728e+01,  ...,  5.8639e+00,\n",
       "          2.6707e+00, -1.5843e+01]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[-1]['layer_last_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0482e-01,  4.1662e-01, -1.7881e-02,  ...,  1.4470e-01,\n",
       "          1.9332e-01, -5.1653e-02],\n",
       "        [-1.2791e-01,  8.6794e-02,  1.1645e-02,  ...,  2.4013e-02,\n",
       "          6.9477e-02, -1.4329e-01],\n",
       "        [ 5.5292e-02, -1.2492e-01, -1.9515e-02,  ...,  7.1906e-03,\n",
       "          4.3979e-02, -4.8419e-02],\n",
       "        ...,\n",
       "        [ 8.2082e+01,  6.4029e+01,  1.7222e+00,  ...,  2.3686e+01,\n",
       "         -8.0630e+00, -7.2619e+01],\n",
       "        [ 1.2726e+02,  8.3658e+01, -4.8099e+01,  ..., -3.4301e+00,\n",
       "         -1.3376e+01, -1.1925e+02],\n",
       "        [ 2.4463e+00,  2.1555e+00, -8.4142e-01,  ..., -1.2124e+00,\n",
       "         -1.1269e+00, -6.6522e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[-1]['layer_mean_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=tokenizer(embeddings['text'][-1], return_tensors='pt').input_ids.to(model.device), output_hidden_states=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3045e-01,  5.2381e-01,  8.2463e-01,  ..., -1.1306e-01,\n",
       "          3.3893e-02, -2.3120e-02],\n",
       "        [-1.0142e-01,  2.8770e-01,  3.6419e-01,  ...,  6.0856e-02,\n",
       "         -2.9304e-02, -8.4377e-02],\n",
       "        [-3.7873e-02, -3.0839e-02,  3.2535e-01,  ...,  2.4453e-01,\n",
       "         -9.8545e-02,  5.0027e-02],\n",
       "        ...,\n",
       "        [-5.3461e+01, -3.4155e+00, -2.3939e+02,  ...,  2.5938e+02,\n",
       "          1.3480e+02, -1.9434e+02],\n",
       "        [-3.0611e+02, -3.3730e+01, -4.7096e+02,  ...,  1.6534e+02,\n",
       "          1.7137e+02, -4.1980e+02],\n",
       "        [-9.2746e+00,  6.5281e-02, -1.0728e+01,  ...,  5.8639e+00,\n",
       "          2.6707e+00, -1.5843e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0482e-01,  4.1662e-01, -1.7881e-02,  ...,  1.4470e-01,\n",
       "          1.9332e-01, -5.1653e-02],\n",
       "        [-1.2791e-01,  8.6794e-02,  1.1645e-02,  ...,  2.4013e-02,\n",
       "          6.9477e-02, -1.4329e-01],\n",
       "        [ 5.5292e-02, -1.2492e-01, -1.9515e-02,  ...,  7.1906e-03,\n",
       "          4.3979e-02, -4.8419e-02],\n",
       "        ...,\n",
       "        [ 8.2082e+01,  6.4029e+01,  1.7222e+00,  ...,  2.3686e+01,\n",
       "         -8.0630e+00, -7.2619e+01],\n",
       "        [ 1.2726e+02,  8.3658e+01, -4.8099e+01,  ..., -3.4301e+00,\n",
       "         -1.3376e+01, -1.1925e+02],\n",
       "        [ 2.4463e+00,  2.1555e+00, -8.4142e-01,  ..., -1.2124e+00,\n",
       "         -1.1269e+00, -6.6522e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].float().mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5535,  1.9119,  0.9343,  ...,  1.7148, -1.6708,  1.1468],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).mean(2)[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings2['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0019)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.stack(model_output.hidden_states).float().mean(2)[-1,0].cpu() - train_embeddings3['layer_mean_embeddings'][-1,-1]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5547, -1.5234,  2.7812,  ..., -0.6445,  0.6094, -1.0469],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings[0,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 98, 2048])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_output.hidden_states).permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00, -6.1035e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5259e-05,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.0518e-05,  1.5259e-05,  2.4414e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.0518e-05],\n",
       "        ...,\n",
       "        [-9.7656e-04,  0.0000e+00,  4.8828e-04,  ..., -9.7656e-04,\n",
       "         -4.8828e-04,  0.0000e+00],\n",
       "        [ 0.0000e+00,  9.7656e-04, -1.2207e-04,  ...,  1.9531e-03,\n",
       "         -1.2207e-03,  8.5449e-04],\n",
       "        [-3.9062e-03,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  3.9062e-03]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings['layer_mean_embeddings'][-1] - torch.stack(model_output.hidden_states)[:,0].mean(1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 161, 2048])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1414e-02, -2.2278e-03, -1.1292e-03,  ..., -1.7944e-02,\n",
       "         -2.6855e-03, -2.1729e-02],\n",
       "        [ 2.0142e-03,  4.8828e-03, -1.0400e-01,  ..., -2.1729e-02,\n",
       "         -2.8809e-02, -3.7354e-02],\n",
       "        [ 7.7515e-03, -5.6152e-02, -1.5430e-01,  ...,  5.4932e-02,\n",
       "         -3.9551e-02, -6.4941e-02],\n",
       "        ...,\n",
       "        [-9.5703e-02,  3.2227e-01, -5.9766e-01,  ..., -7.6562e-01,\n",
       "         -7.1289e-02, -5.2246e-02],\n",
       "        [ 1.9141e-01,  4.3359e-01, -5.6641e-01,  ..., -6.3281e-01,\n",
       "         -9.3262e-02, -6.0547e-02],\n",
       "        [ 2.5312e+00,  3.9062e+00,  2.3730e-01,  ..., -4.7188e+00,\n",
       "         -4.8438e+00, -1.0781e+00]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.9182e-04,  3.4027e-03,  1.2512e-02,  ..., -8.8501e-04,\n",
       "         -6.8970e-03, -5.7068e-03],\n",
       "        [-1.2085e-02,  8.4229e-03, -1.9653e-02,  ...,  9.2773e-03,\n",
       "         -2.4048e-02,  4.2419e-03],\n",
       "        [ 1.1292e-03,  3.9673e-03, -2.4658e-02,  ...,  1.7700e-02,\n",
       "         -3.5400e-02, -3.6926e-03],\n",
       "        ...,\n",
       "        [ 3.4668e-02,  1.6309e-01, -1.3184e-01,  ..., -9.6680e-02,\n",
       "         -1.6724e-02,  7.3730e-02],\n",
       "        [ 9.0332e-02,  2.1484e-01, -7.2754e-02,  ..., -7.2754e-02,\n",
       "         -1.3281e-01, -2.6611e-02],\n",
       "        [ 1.3125e+00,  3.5312e+00,  6.1719e-01,  ..., -2.7969e+00,\n",
       "         -3.2812e+00, -1.4648e-01]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_output.hidden_states)[:,0].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].to(model.device)\n",
    "    attention_mask = tokens['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'model_out': model_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = get_model_out(dataset.take(10))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = out['input_ids'].cpu()\n",
    "attention_mask = out['attention_mask'].cpu()\n",
    "model_out = out['model_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_indices = attention_mask.sum(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(model_out.hidden_states).cpu().permute(1,0,2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_token_embeddings = torch.stack(model_out.hidden_states).permute(1,0,2,3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_last_embedding = input_layer_token_embeddings[torch.arange(input_layer_token_embeddings.shape[0]), :, last_token_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 452, 2048])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 452])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 452])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 452, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.unsqueeze(-1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([315, 279, 146, 452, 160, 218, 361, 208, 177, 216])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_mean_embeddings = (input_layer_token_embeddings * attention_mask.unsqueeze(-1).unsqueeze(1)).sum(2) / attention_mask.sum(1).unsqueeze(-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9073e-03,  3.4180e-03,  1.1902e-02,  ..., -5.9814e-03,\n",
       "          -5.7068e-03, -4.3640e-03],\n",
       "         [-1.2329e-02,  1.1475e-02, -1.5625e-02,  ...,  4.3640e-03,\n",
       "          -1.2085e-02,  1.6098e-03],\n",
       "         [ 1.9531e-03,  6.3782e-03, -1.8555e-02,  ...,  1.0010e-02,\n",
       "          -1.8433e-02, -5.5847e-03],\n",
       "         ...,\n",
       "         [-1.3086e-01, -3.5156e-02, -4.4861e-03,  ..., -1.4258e-01,\n",
       "           3.2959e-02,  9.8633e-02],\n",
       "         [-1.3965e-01,  3.3691e-02,  1.0840e-01,  ..., -1.3867e-01,\n",
       "           2.4261e-03,  1.0107e-01],\n",
       "         [-4.1406e-01,  2.4688e+00,  1.1172e+00,  ..., -2.7031e+00,\n",
       "          -2.7656e+00,  7.1094e-01]],\n",
       "\n",
       "        [[-2.7924e-03,  2.7618e-03,  1.0254e-02,  ..., -3.5400e-03,\n",
       "          -4.1809e-03, -3.6621e-03],\n",
       "         [-1.1230e-02,  1.0254e-02, -2.5635e-02,  ...,  3.9062e-03,\n",
       "          -2.1729e-02,  2.5177e-03],\n",
       "         [ 4.8065e-04,  8.8501e-03, -3.1006e-02,  ...,  1.0986e-02,\n",
       "          -4.1504e-02, -8.0566e-03],\n",
       "         ...,\n",
       "         [ 4.1992e-02,  9.6191e-02, -5.5664e-02,  ..., -1.6406e-01,\n",
       "           1.1426e-01,  1.3672e-01],\n",
       "         [ 3.5645e-02,  1.1865e-01,  6.8359e-02,  ..., -2.0605e-01,\n",
       "           1.1865e-01,  1.2695e-01],\n",
       "         [ 2.6953e-01,  2.5781e+00,  9.6484e-01,  ..., -3.0938e+00,\n",
       "          -1.5859e+00,  8.5547e-01]],\n",
       "\n",
       "        [[-2.5330e-03,  5.2490e-03,  1.1047e-02,  ..., -6.4087e-03,\n",
       "          -2.4872e-03, -5.8289e-03],\n",
       "         [-1.0498e-02,  8.1177e-03, -2.9053e-02,  ..., -8.2779e-04,\n",
       "          -1.9409e-02,  2.4033e-04],\n",
       "         [ 3.1891e-03,  2.2278e-03, -3.3936e-02,  ...,  1.2024e-02,\n",
       "          -3.5645e-02, -4.9438e-03],\n",
       "         ...,\n",
       "         [-1.0547e-01, -2.9907e-02, -1.6992e-01,  ..., -1.2402e-01,\n",
       "           6.9824e-02,  1.0156e-01],\n",
       "         [-1.2158e-01, -1.5106e-03, -9.1309e-02,  ..., -6.2988e-02,\n",
       "          -1.7334e-02, -2.6001e-02],\n",
       "         [-2.8906e-01,  2.8438e+00,  3.9453e-01,  ..., -1.9297e+00,\n",
       "          -3.2969e+00, -8.6914e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.5695e-04,  3.0060e-03,  1.2817e-02,  ..., -4.0894e-03,\n",
       "          -3.4790e-03, -6.3171e-03],\n",
       "         [-8.6670e-03,  8.7280e-03, -2.9541e-02,  ...,  5.7983e-04,\n",
       "          -1.6113e-02,  2.5177e-03],\n",
       "         [-6.3705e-04,  6.6223e-03, -4.0283e-02,  ...,  1.1841e-02,\n",
       "          -2.3193e-02, -3.2501e-03],\n",
       "         ...,\n",
       "         [-1.1914e-01,  5.8838e-02, -1.1133e-01,  ..., -1.3672e-01,\n",
       "           9.6191e-02,  1.2500e-01],\n",
       "         [-1.4160e-01,  6.3965e-02,  2.2736e-03,  ..., -9.0332e-02,\n",
       "           5.9570e-02,  5.3711e-02],\n",
       "         [-2.0703e-01,  2.3281e+00,  5.5469e-01,  ..., -2.0781e+00,\n",
       "          -2.1875e+00,  9.2969e-01]],\n",
       "\n",
       "        [[-1.1978e-03,  3.7689e-03,  1.3855e-02,  ..., -1.0071e-03,\n",
       "          -4.9438e-03, -3.2501e-03],\n",
       "         [-8.8501e-03,  8.7280e-03, -1.4893e-02,  ...,  5.0049e-03,\n",
       "          -1.9775e-02,  5.0659e-03],\n",
       "         [ 3.6926e-03,  9.1553e-03, -1.5869e-02,  ...,  7.2327e-03,\n",
       "          -2.1240e-02,  2.9602e-03],\n",
       "         ...,\n",
       "         [ 4.5471e-03,  1.0889e-01,  3.2227e-02,  ...,  3.6621e-02,\n",
       "           2.9175e-02,  2.2070e-01],\n",
       "         [ 5.2246e-02,  2.0215e-01,  1.2891e-01,  ...,  6.6406e-02,\n",
       "          -2.4658e-02,  9.8633e-02],\n",
       "         [ 4.2480e-02,  3.3438e+00,  8.5938e-01,  ..., -1.7266e+00,\n",
       "          -2.3438e+00,  1.2422e+00]],\n",
       "\n",
       "        [[-4.1199e-03,  6.6223e-03,  8.1177e-03,  ..., -2.1362e-03,\n",
       "          -3.1128e-03, -3.9062e-03],\n",
       "         [-1.3245e-02,  8.3618e-03, -1.3428e-02,  ...,  6.3782e-03,\n",
       "          -1.9409e-02,  5.6152e-03],\n",
       "         [-3.4180e-03,  3.9368e-03, -1.8188e-02,  ...,  2.2583e-02,\n",
       "          -3.4180e-02, -5.3101e-03],\n",
       "         ...,\n",
       "         [-1.0400e-01,  7.5195e-02, -1.7456e-02,  ..., -2.3242e-01,\n",
       "          -1.9043e-02,  9.4238e-02],\n",
       "         [-1.5723e-01,  9.9609e-02,  8.9355e-02,  ..., -2.0996e-01,\n",
       "          -9.4238e-02,  1.1841e-02],\n",
       "         [-1.4941e-01,  2.7969e+00,  7.5000e-01,  ..., -3.2188e+00,\n",
       "          -2.9062e+00,  3.3203e-02]]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 10, 452, 2048])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_layer_token_embeddings.permute(1,0,2,3) * attention_mask.unsqueeze(-1)).mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17, 2048])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_mean_embeddings = ((torch.stack(model_out.hidden_states)[:, torch.arange(input_ids.size(0))] * attention_mask.unsqueeze(-1)).sum(2) / attention_mask.sum(-1).unsqueeze(-1)).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
